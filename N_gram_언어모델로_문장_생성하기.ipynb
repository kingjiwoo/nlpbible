{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYTA38b8vISKPJ79/kd+gQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kingjiwoo/nlpbible/blob/main/N_gram_%EC%96%B8%EC%96%B4%EB%AA%A8%EB%8D%B8%EB%A1%9C_%EB%AC%B8%EC%9E%A5_%EC%83%9D%EC%84%B1%ED%95%98%EA%B8%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNgyIAKCviNv",
        "outputId": "ad779413-a104-4d4b-ee87-40cd2169b774"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.4.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.3)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.1)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.1 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 실습명: N-gram 언어 모델로 문장 생성하기\n",
        "2. 실습 목적 및 설명\n",
        "- 파이썬의 NLTK 패키지를 이용하여 N-gram 언어 모델을 구축한다.\n",
        "- 네이버에서 제공하는 nsmc 영화리뷰 데이터셋을 이용하여 문장을 생성한다.\n",
        "\n",
        "3. flow\n",
        "- 입력 텍스트를 N-gram으로 바꿈\n",
        "- 단어의 빈도를 측정,\n",
        "- 조건부 확률을 측정\n",
        "- 단어열을 예측하고 생성하는 모델 구축"
      ],
      "metadata": {
        "id": "15E3RsWxtVlT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ngkCGq_LVPxN"
      },
      "outputs": [],
      "source": [
        "# import modules\n",
        "\n",
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from nltk import word_tokenize\n",
        "from nltk import ConditionalFreqDist\n",
        "from nltk.probability import ConditionalProbDist, MLEProbDist\n",
        "import codecs\n",
        "from konlpy.tag import Okt\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NLTK 패키지를 이용하여 입력 텍스트를 N-gram 형태로 변환한다.\n",
        "sentence = \"나는 매일 아침 지하철을 탄다\""
      ],
      "metadata": {
        "id": "N3puFkrlvZWs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#NLTK 사용을 위해 선행 패키지를 설치한다.\n",
        "nltk.download('punkt')\n",
        "\n",
        "# 입력 텍스트를 띄어쓰기 기준으로 토큰화한다.\n",
        "tokens = word_tokenize(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZUpGmekwBLv",
        "outputId": "af4e73fe-600d-4caa-eec4-aae3192a1c9b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 토큰 확인\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9dNwG20wRaw",
        "outputId": "355012b3-a2e6-4ec6-b6f5-2e13fc74b71b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['나는', '매일', '아침', '지하철을', '탄다']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 한국어의 단어는 띄어쓰기를 기준으로 하지 않기 때문에 konlpy를 이용해 형태소를 기준으로 토큰화한다.\n",
        "\n",
        "tagger = Okt()\n",
        "\n",
        "def tokenize(text):\n",
        "    tokens = [ '/'.join(t) for t in tagger.pos(text)]\n",
        "    return tokens\n",
        "\n",
        "tokens = tokenize(sentence)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lbZ3nwlwVd0",
        "outputId": "d102f998-85eb-412d-9d0f-87fe3e571447"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['나/Noun', '는/Josa', '매일/Noun', '아침/Noun', '지하철/Noun', '을/Josa', '탄다/Verb']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 토큰을 N-gram의 형태로 바꾸어준다.\n",
        "# ngrams 함수의 두 번째 인자로 N값을 지정할 수 있다.\n",
        "bigram = ngrams(tokens, 2)\n",
        "trigram = ngrams(tokens, 3)"
      ],
      "metadata": {
        "id": "nJDYhVQCxEAl"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# N-gram을 출력해본다.\n",
        "print(\"bigram:\")\n",
        "for b in bigram:\n",
        "    print(b)\n",
        "\n",
        "print(\"\\ntrigram:\")\n",
        "for t in trigram:\n",
        "    print(t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlA8d--qxQZE",
        "outputId": "2de2db67-2c94-4acb-ea43-f8e0b4adebca"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bigram:\n",
            "('나/Noun', '는/Josa')\n",
            "('는/Josa', '매일/Noun')\n",
            "('매일/Noun', '아침/Noun')\n",
            "('아침/Noun', '지하철/Noun')\n",
            "('지하철/Noun', '을/Josa')\n",
            "('을/Josa', '탄다/Verb')\n",
            "\n",
            "trigram:\n",
            "('나/Noun', '는/Josa', '매일/Noun')\n",
            "('는/Josa', '매일/Noun', '아침/Noun')\n",
            "('매일/Noun', '아침/Noun', '지하철/Noun')\n",
            "('아침/Noun', '지하철/Noun', '을/Josa')\n",
            "('지하철/Noun', '을/Josa', '탄다/Verb')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# padding을 통해 입력 데이터에 문장의 시작과 끝을 알리는 토큰을 추가한다.\n",
        "bigram = ngrams(tokens, 2, pad_left = True, pad_right= True, left_pad_symbol=\"<s>\", right_pad_symbol=\"<\\s>\")\n",
        "print(\"bigrams with padding:\")\n",
        "for b in bigram:\n",
        "    print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8Iznw6Nycto",
        "outputId": "e1c4eefd-7b25-4285-aa91-b8c1ac54cbe4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bigrams with padding:\n",
            "('<s>', '나/Noun')\n",
            "('나/Noun', '는/Josa')\n",
            "('는/Josa', '매일/Noun')\n",
            "('매일/Noun', '아침/Noun')\n",
            "('아침/Noun', '지하철/Noun')\n",
            "('지하철/Noun', '을/Josa')\n",
            "('을/Josa', '탄다/Verb')\n",
            "('탄다/Verb', '<\\\\s>')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장 생성을 위하여 네이버 영화 리뷰 데이터셋을 다운로드한다.\n",
        "%%time\n",
        "!wget -nc -q https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1FYKVtb0RrN",
        "outputId": "818ea99b-cc71-408f-8030-caa6a81b7ce3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 6.88 ms, sys: 0 ns, total: 6.88 ms\n",
            "Wall time: 406 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#다운로드 받은 데이터셋을 읽고 인덱스와 라벨을 제외한 텍스트 부분만 가져온다.\n",
        "# codecs 패키지는 대용량 파일을 조금씩 읽을 수 있게 해준다.\n",
        "with codecs.open(\"ratings_train.txt\", encoding='utf-8') as f:\n",
        "    data = [line.split('\\t') for line in f.read().splitlines()]\n",
        "    data = data[1:] # header 제외\n",
        "print(\"데이터셋:\", data[:10])\n",
        "\n",
        "docs = [row[1] for row in data] # 텍스트 부분만가져옴\n",
        "print(\"텍스트 데이터:\", docs[:5])\n",
        "print(\"문장 개수 \", len(docs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltJ13EaY0Zyq",
        "outputId": "cd16a2a1-1418-4477-b7cf-261522a72f24"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "데이터셋: [['9976970', '아 더빙.. 진짜 짜증나네요 목소리', '0'], ['3819312', '흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나', '1'], ['10265843', '너무재밓었다그래서보는것을추천한다', '0'], ['9045019', '교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정', '0'], ['6483659', '사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다', '1'], ['5403919', '막 걸음마 뗀 3세부터 초등학교 1학년생인 8살용영화.ㅋㅋㅋ...별반개도 아까움.', '0'], ['7797314', '원작의 긴장감을 제대로 살려내지못했다.', '0'], ['9443947', '별 반개도 아깝다 욕나온다 이응경 길용우 연기생활이몇년인지..정말 발로해도 그것보단 낫겟다 납치.감금만반복반복..이드라마는 가족도없다 연기못하는사람만모엿네', '0'], ['7156791', '액션이 없는데도 재미 있는 몇안되는 영화', '1'], ['5912145', '왜케 평점이 낮은건데? 꽤 볼만한데.. 헐리우드식 화려함에만 너무 길들여져 있나?', '1']]\n",
            "텍스트 데이터: ['아 더빙.. 진짜 짜증나네요 목소리', '흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나', '너무재밓었다그래서보는것을추천한다', '교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정', '사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다']\n",
            "문장 개수  150000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 토큰화한 텍스트 데이터의 bigram을 모두 리스트에 추가한다.\n",
        "sentences = []\n",
        "for d in tqdm(docs):\n",
        "    tokens = tokenize(d)\n",
        "    bigram = ngrams(tokens, 2, pad_left = True, pad_right= True, left_pad_symbol=\"<s>\", right_pad_symbol=\"<\\s>\")\n",
        "    sentences += [t for t in bigram]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-Ihp7aP1zhm",
        "outputId": "72536949-c7d4-477d-bbca-5f1a37717b6b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 150000/150000 [08:42<00:00, 287.01it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentences[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYAFaMvz20_P",
        "outputId": "9e280d10-c4ee-467a-db03-6423eb9b9def"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('<s>', '아/Exclamation'), ('아/Exclamation', '더빙/Noun'), ('더빙/Noun', '../Punctuation'), ('../Punctuation', '진짜/Noun'), ('진짜/Noun', '짜증나네요/Adjective'), ('짜증나네요/Adjective', '목소리/Noun'), ('목소리/Noun', '<\\\\s>'), ('<s>', '흠/Noun'), ('흠/Noun', '.../Punctuation'), ('.../Punctuation', '포스터/Noun')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sentences에 대한 단어별 등장 빈도를 측정한다.\n",
        "cfd = ConditionalFreqDist(sentences)"
      ],
      "metadata": {
        "id": "7Ff4RVCi3Ctv"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# \"<s>\" 다음에 가장 많이 오는 단어(즉, 문장 맨 처음에 가장 많이 오는 단어) 5개를 출력해본다.\n",
        "print(cfd[\"<s>\"].most_common(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rX_Dh9G3R0j",
        "outputId": "2d9e2e58-9b4b-4627-d9f5-8306c154fdf2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('정말/Noun', 2718), ('이/Noun', 2371), ('진짜/Noun', 2232), ('이/Determiner', 2115), ('영화/Noun', 2069)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 주어진 토큰(c) 다음에 가장 많이 등장하는 n개의 단어를 반환하는 함수를 만든다.\n",
        "def most_common(c, n, pos=None):\n",
        "    if pos is None:\n",
        "        return cfd[tokenize(c)[0]].most_common(n)\n",
        "    else:\n",
        "        return cfd[\"/\".join([c, pos])].most_common(n)\n",
        "\n",
        "print(most_common(\"나\", 10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76dx4VwO4-6K",
        "outputId": "b1c1a845-1895-4395-d9a5-a3462b7c5d79"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('는/Josa', 831), ('의/Josa', 339), ('만/Josa', 213), ('에게/Josa', 148), ('에겐/Josa', 84), ('랑/Josa', 81), ('한테/Josa', 50), ('참/Verb', 45), ('이/Determiner', 44), ('와도/Josa', 43)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어별 등장 빈도를 기반으로 조건부확률을 추정한다.\n",
        "cpd = ConditionalProbDist(cfd, MLEProbDist)"
      ],
      "metadata": {
        "id": "JUMi3cbu63hj"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\".\" 다음에 \"</s>\" 가 올 확률을 출력한다.\n",
        "print(cpd[tokenize('.')[0]].prob(\"<\\s>\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltbD4ct07tD6",
        "outputId": "e64b1d8c-3944-4a27-d08b-faa9754485a9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.39102658679807606\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 토큰 c 다음에 토큰 w가 bigram으로 함께 등장할 확률을 구한다.\n",
        "def bigram_prob(c,w):\n",
        "    context = tokenize(c)[0]\n",
        "    word = tokenize(w)[0]\n",
        "    return cpd[context].prob(word)\n",
        "\n",
        "print(bigram_prob(\"영화\", \"이\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNRlowfe8FGb",
        "outputId": "04b2aa01-16c9-4dbe-d827-fb81f5e277a5"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.00015767585785521414\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 조건부 확률을 알게되면 가장 확률이 높은 토큰열을 토대로 문장을 생성할 수 있다.\n",
        "def generate_sentence (seed=None, debug=False):\n",
        "    if seed is not None:\n",
        "        import random\n",
        "        random.seed(seed)\n",
        "    c = \"<s>\"\n",
        "    sentence = []\n",
        "    while True:\n",
        "        if c not in cpd:\n",
        "            break\n",
        "        w = cpd[c].generate()\n",
        "        if w == \"<\\s>\":\n",
        "            break\n",
        "        word = w.split(\"/\")[0]\n",
        "        pos = w.split(\"/\")[1]\n",
        "\n",
        "        # 조사, 어미 등을 제외하고 각 토큰은 띄어쓰기로 구분하여 생성한다.\n",
        "        if c == \"\":\n",
        "            sentence.append(word.title())\n",
        "        elif c in [\"`\", \"\\\"\",\"'\",\"(\"]:\n",
        "            sentence.append(word)\n",
        "        elif word in [\"'\", \".\", \",\", \")\", \":\", \";\", \"?\"]:\n",
        "            sentence.append(word)\n",
        "        elif pos in [\"Josa\", \"Punctuation\", \"Suffix\"]:\n",
        "            sentence.append(word)\n",
        "        elif w in [\"임/Noun\", \"것/Noun\", \"는걸/Noun\", \"릴때/Noun\",\n",
        "                    \"되다/Verb\", \"이다/Verb\", \"하다/Verb\", \"이다/Adjective\"]:\n",
        "            sentence.append(word)\n",
        "        else:\n",
        "            sentence.append(\" \" + word)\n",
        "        c = w\n",
        "\n",
        "        if debug:\n",
        "            print(w)\n",
        "\n",
        "    return \"\".join(sentence)\n",
        "\n",
        "print(generate_sentence(2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8smSXHu8QLH",
        "outputId": "7042eda4-d9a0-476b-9074-1363af489ad9"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 도리까지 본 영화 너무... 뭔가.. 최고네요. 하지만.. 눈물 낫다는건 또 영화에 들지 않는다. 근데 뭐야 어떻게 그렇게 착했던 윤재랑은 에바 그린 드레스 소리 듣는거임\"\"\" 에리 욧의 미모로 합성 한 가수 노래와 흥행 놓친 영화다. 사투리 연기 하나 없는 ‘ 스피드 감 넘치는 스릴 넘치는 연기를 이해 되지 못 하시는 분보다 훨 재밌구만 평점을 망처 놓은 듯하다. 영화 보는이로 하여금 불편함을 느꼇을듯\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7fOgrP0H9yXt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}